# 《Linux内核设计与实现》笔记 ✏️⭐️

> **[参考：《Linux内核设计与实现》读书笔记](https://www.cnblogs.com/wang_yb/p/3514730.html)**



# 第一章 Linux内核简介

### 1. 单内核和微内核

|            | **原理**                                                     | **优势**                                                     | **劣势**                                                   |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---------------------------------------------------------- |
| **单内核** | 整体上作为一个单独的大过程来实现，整个内核都在一个大内核地址空间上运行。 | 1. 简单。 2. 高效：所有内核都在一个大的地址空间上，所以内核各个功能之间的调用和调用函数类似，几乎没有性能开销。 | 一个功能的崩溃会导致整个内核无法使用。                     |
| **微内核** | 内核按功能被划分成多个独立的过程。每个过程独立的运行在自己的地址空间上。 | 1. 安全：内核的各种服务独立运行，一种服务挂了不会影响其他服务。 | 内核各个服务之间通过进程间通信互通消息，比较复杂且效率低。 |

因为 IPC 机制的开销多于函数调用，又因为会涉及内核空间与用户空间的上下文切换，因此，消息传递需要一定的周期，而单内核中简单的函数调用没有这些开销。

Linux的内核虽然是基于单内核的，运行在单独的内核地址空间上。但是经过这么多年的发展，也具备微内核的一些特征。（体现了Linux实用至上的原则）

主要有以下特征：

1. 模块化设计，支持动态加载内核模块
2. 支持对称多处理（SMP）
3. 内核可以抢占（preemptive），允许内核运行的任务有优先执行的能力
4. 支持内核线程，不区分线程和进程



### 2. 内核版本号

内核的版本号主要有四个数组组成。比如版本号：2.6.26.1 其中：

2 - **主版本号**

6 - **从版本号或副版本号**

26 - 修订版本号

1 - 稳定版本号

副版本号表示这个版本是稳定版（**偶数**）还是开发版（**奇数**），上面例子中的版本号是稳定版。

稳定的版本可用于企业级环境。

修订版本号的升级包括BUG修正，新的驱动以及新的特性的追加。

稳定版本号主要是一些关键性BUG的修改。



# 第二章 从内核出发

### 1. 获取内核源码

内核是开源的，所有获取源码特别方便，参照以下的网址，可以通过git或者直接下载压缩好的源码包。

[http://www.kernel.org](http://www.kernel.org/)



### 2. 内核源码的结构

| **目录**      | **说明**                            |
| ------------- | ----------------------------------- |
| arch          | 特定体系结构的代码                  |
| block         | 块设备I/O层                         |
| crypo         | 加密API                             |
| Documentation | 内核源码文档                        |
| drivers       | 设备驱动程序                        |
| firmware      | 使用某些驱动程序而需要的设备固件    |
| fs            | VFS和各种文件系统                   |
| include       | 内核头文件                          |
| init          | 内核引导和初始化                    |
| ipc           | 进程间通信代码                      |
| kernel        | 像调度程序这样的核心子系统          |
| lib           | 同样内核函数                        |
| mm            | 内存管理子系统和VM                  |
| net           | 网络子系统                          |
| samples       | 示例，示范代码                      |
| scripts       | 编译内核所用的脚本                  |
| security      | Linux 安全模块                      |
| sound         | 语音子系统                          |
| usr           | 早期用户空间代码（所谓的initramfs） |
| tools         | 在Linux开发中有用的工具             |
| virt          | 虚拟化基础结构                      |



### 3. 编译内核的方法

还未实际尝试过手动编译内核，只是用yum更新过内核。这部分等以后手动编译过再补上。

安装新的内核后，重启时会提示进入哪个内核。当多次安装新的内核后，启动列表会很长（因为有很多版本的内核），显得不是很方便。



下面介绍3种删除那些不用的内核的方法：(是如何安装的就选择相应的删除方法)

- rpm 删除法

    rpm -qa | grep kernel* (查找所有linux内核版本)
    rpm -e kernel-(想要删除的版本)

- yum 删除法

    yum remove kernel-(要删除的版本)

- 手动删除

    删除/lib/modules/目录下不需要的内核库文件
    删除/usr/src/kernel/目录下不需要的内核源码
    删除/boot目录下启动的核心档案和内核映像
    更改grub的配置，删除不需要的内核启动列表

 

### 4. 内核开发的特点

#### 4.1 无标准C库

为了保证内核的小和高效，内核开发中不能使用C标准库，所以连最常用的printf函数也没有，但是还好有个printk函数来代替。



#### 4.2 使用GNU C

因为使用GNU C，所有内核中常使用GNU C中的一些扩展：

- **内联函数**

  内联函数在编译时会在它被调用的地方展开，减少了函数调用的开销，性能较好。但是，频繁的使用内联函数也会使代码变长，从而在运行时占用更多的内存。

  所以内联函数使用时最好要满足以下几点：函数较小，会被反复调用，对程序的时间要求比较严格。

  内联函数示例：static **inline** void sample();

- **内联汇编**

  内联汇编用于偏近底层或对执行时间严格要求的地方。示例如下：

    ```C
    unsigned int low, high;
    asm volatile("rdtsc" : "=a" (low), "=d" (high));
    /* low 和 high 分别包含64位时间戳的低32位和高32位 */
    ```

- **分支声明**

  如果能事先判断一个if语句时经常为真还是经常为假，那么可以用unlikely和likely来优化这段判断的代码。

    ```C
    /* 如果error在绝大多数情况下为0(假) */
    if (unlikely(error)) {
        /* ... */
    }

    /* 如果success在绝大多数情况下不为0(真) */
    if (likely(success)) {
        /* ... */
    }
    ```



#### 4.3 没有内存保护

因为内核是最低层的程序，所以如果内核访问的非法内存，那么整个系统都会挂掉！所以内核开发的风险比用户程序开发的风险要大。

内核中的内存是不分页的，每用一个字节的内存，物理内存就少一个字节。所以内核中使用内存一定要谨慎。



#### 4.4 不使用浮点数

内核不能完美的支持浮点操作，使用浮点数时，需要人工保存和恢复浮点寄存器及其他一些繁琐的操作。



#### 4.5 内核栈容积小且固定

内核栈的大小有编译内核时决定的，对于不用的体系结构，内核栈的大小虽然不一样，但都是固定的。

查看内核栈大小的方法：

```C
ulimit -a | grep "stack size"
```



#### 4.6 同步和并发

Linux是多用户的操作系统，所以必须处理好同步和并发操作，防止因竞争而出现死锁。

内核很容易产生竞争条件。和单线程的用户空间程序不同，内核的许多特性都要求能够并发地访问共享数据，这就要求有同步机制以保证不出现竞争条件，特别是：

- **Linux 是抢占多任务操作系统**。内核的进程调度程序即兴对进程进行调度和重新调度。内核必须和这些任务同步。
- **Linux 内核支持对称多处理器系统 (SMP)** 。所以，如果没有适当的保护，同时在两个或两个以上的处理器上执行的内核代码很可能会同时访问共享的同一个资源。
- **中断是异步到来的**，完全不顾及当前正在执行的代码。也就是说，如果不加以适当的保护，中断完全有可能在代码访问资源的时候到来，这样，中段处理程序就有可能访问同一资源。
- **Linux 内核可以抢占**。所以，如果不加以适当的保护，内核中一段正在执行的代码可能会被另外一段代码抢占，从而有可能导致儿段代码同时访问相同的资源。

常用的解决竞争的办法是**自旋锁和信号量**。



#### 4.7 可移植性

Linux内核可用于不用的体现结构，支持多种硬件。所以开发时要时刻注意可移植性，尽量使用体系结构无关的代码。



# 第三章 进程管理

### 1. 进程

**程序本身并不是进程，进程是处于执行期的程序以及相关的资源的总称。**

- 可能存在两个或多个不同的进程执行的是同一个程序。
- 两个或两个以上并存的进程还可以共享许多诸如打开的文件、地址空间之类的资源。



进程和线程是程序运行时状态，是动态变化的，进程和线程的管理操作(比如，创建，销毁等)都由内核来实现的。

Linux中**不严格区分进程和线程**，对Linux而言线程不过是一种特殊的进程。

 

现代操作系统中，进程提供2种虚拟机制：虚拟处理器和虚拟内存

- 虚拟处理器给进程一种假象，让这些进程觉得自己在独享处理器。
- 虚拟内存让进程在分配和管理内存时觉得自己拥有整个系统的所有内存资源。

每个进程有独立的虚拟处理器和虚拟内存

**在进程中的各个线程之间可以共享虚拟内存，但每个都拥有各自的虚拟处理器。**

 

进程的创建与退出：

1. 进程在创建它的时刻开始存活，在 Linux 系统中，这通常是调用 fork()系统的结果，该系统调用通过复制一个现有进程来创建一个全新的进程。调用 fork()的进程称为父进程，新产生的进程称为子进程。在该调用结束时，在返回点这个相同位置上，父进程恢复执行，子进程开始执行。 **fork()系统调用从内核返回两次：一次回到父进程，另一次回到新产生的子进程。**

2. 创建新的进程都是为了立即执行新的、不同的程序，而接着调用 exec() 这组函数就可以创建新的地址空间，并把新的程序载入其中。在现代 Linux 内核中， fork()实际上是由clone()系统调用实现的。

3. 最终，程序通过 exit()系统调用退出执行。这个函数会终结进程并将其占用的资源释放掉。进程退出执行后被设置为僵死状态，直到它的父进程调用 wait()或 waitpid()为止。



内核中进程的信息主要保存在task_struct中(include/linux/sched.h)

进程标识PID和线程标识TID对于同一个进程或线程来说都是相等的。

Linux中可以用ps命令查看所有进程的信息：

```sh
ps -eo pid,tid,ppid,comm
```

 

### 2. 进程描述符及任务结构

内核把进程的列表存放在叫做**任务队列** (task list) 的**双向循环链表**中。链表中的每一项都是类型为 task_struct 称为**进程描述符** (process descriptor) 的结构，该结构定义在 include/linux/sched.h文件中。进程描述符中包含一个具体进程的所有信息。

进程描述符中包含的数据能完整地描述一个正在执行的程序：它打开的文件，进程的地址空间，挂起的信号，进程的状态，等等。

<div align="center"> <img src="https://gitee.com//MrRen-sdhm/Images/raw/master/img/20200523212758.png" width="400px" /> </div> 



#### 2.1 分配进程描述符

Linux 通过 **slab 分配器**分配 task_struct 结构，这样能达到对象复用和缓存着色 (cache coloring) 的目的。

在2.6版本之后用 slab 分配器动态生成 task_struct，所以只需在栈底（对于向下增长的栈来说）或栈顶（对于向上增长的栈来说）创建一个新的结构 struct thread_ info。

在 x86 上， struct thread_ info 在文件 <asm/tbread_info.b> 中定义如下：

```C
struct thread_info { 
    struct task_struct *task; 
    struct exec_domain *exec_domain; 
    __u32 flags; 
    __u32 status; 
    __u32 cpu; 
    int preempt_count; 
    mm_segment_t addr_limit; 
    struct restart_block restart_block; 
    void *sysenter_return; 
    int uaccess err; 
} ; 
```

<div align="center"> <img src="https://gitee.com//MrRen-sdhm/Images/raw/master/img/20200523213612.png" width="400px" />  </div> 

每个任务的 thread_info 结构在它的**内核栈的尾端**分配。结构中 task 域中存放的是指向该任务实际 task_struct 的指针。



#### 2.2 进程描述符的存放

内核中大部分处理进程的代码都是直接通过 task_struct 进行的。因此，通过 current 宏查找到当前正在运行进程的进程描述符的速度就显得尤为重要。

硬件体系结构不同，该宏的实现也不同，它必须针对专门的硬件体系结构做处理。有的硬件体系结构可以拿出一个专门寄存器来存放指向当前进程 task_struct 的指针，用千加快访问速度。而有些像 x86 这样的体系结构（其寄存器并不富余），就只能在内核栈的尾端创建 thread_info 结构，通过计算偏移间接地查找 task_struct 结构。



#### 2.3 进程状态

进程描述符中的 **state 域**描述了进程的当前状态 。系统中的每个进程都必然处于五种进程状态中的一种。

- TASK_RUNNING （**运行**）一 进程是可执行的；**它或者正在执行，或者在运行队列中等待执行**。这是进程在用户空间中执行的唯一可能的状态；这种状态也可以应用到内核空间中正在执行的进程。
- TASK_INTERRUPTIBLE （**可中断**）一 进程正在睡眠（也就是说它**被阻塞**），等待某些条件的达成。一且这些条件达成，内核就会把进程状态设置为运行。处千此状态的进程也会因为接收到信号而提前被唤醒井随时准备投入运行。
- TASK_ UNINTERRUPTIBLE （**不可中断**）一 除了**就算是接收到信号也不会被唤醒或准备投入运行**外，这个状态与可打断状态相同。这个状态通常在进程必须在等待时不受干扰或等待事件很快就会发生时出现。由于处于此状态的任务对信号不做响应，所以较之可中断状态 , 使用得较少。
- TASK_TRACED （**被跟踪**）— 被其他进程跟踪的进程，例如通过 ptrace 对调试程序进行跟踪。
- TASK_STOPPED （**停止**）— 进程停止执行；进程没有投入运行也不能投人运行。通常这种状态发生在接收到 SIGSTOP 、 SIGTSTP 、 SIGTTIN 、 SIGTTOU 等信号的时候。此外，在调试期间接收到任何信号，都会使进程进入这种状态。

<div align="center"> <img src="https://gitee.com//MrRen-sdhm/Images/raw/master/img/20200523211906.png" width="500px" /> </div>

进程的各个状态之间的转化构成了进程的整个生命周期。



#### 2.4 设置当前进程状态

内核经常需要调整某个进程的状态。这时最好使用 set_task_state(task, state) 函数：

```C
set_task_state(task, state); /*将任务task的状态设置为state*/
```

该函数将指定的进程设置为指定的状态。必要的时候，它会设置内存屏障来强制其他处理器作重新排序。否则，它等价于：

```C
task->state = state;
```



#### 2.5 进程上下文

一般程序在用户空间执行。**当一个程序执行了系统调用或者触发了某个异常，它就陷入了内核空间。此时，我们称内核「代表进程执行」并处于进程上下文中**。在此上下文中 current 宏是有效的。除非在此间隙有更高优先级的进程需要执行并由调度器做出了相应调整，否则在内核退出的时候，程序恢复在用户空间会继续执行。

系统调用和异常处理程序是对内核明确定义的接口。进程只有通过这些接口才能陷入内核执行——对内核的所有访间都必须通过这些接口。



#### 2.6 进程家族树

 Linux 系统中进程之间存在一个明显的继承关系，**所有的进程都是 PID 为 1 的 init 进程的后代**。**内核在系统启动的最后阶段启动 init 进程**。

init 进程读取系统的初始化脚本 (initscript) 并执行其他的相关程序，最终完成系统启动的整个过程。

系统中的每个进程必有一个父进程，相应的，每个进程也可以拥有零个或多个子进程，每个 task_struct 都
包含一个指向其父进程 tast_struct 的 **parent 指针**，还包含一个称为 **children 的子进程链表**。

init 进程的进程描述符是作为 init_task 静态分配的。



### 3. 进程的创建

Linux中创建进程与其他系统有个主要区别，**Linux中创建进程分2步：fork() 和 exec()**，而其他系统通常提供spawn() 函数创建进程并读入可执行文件，然后开始执行。

- fork: 通过拷贝当前进程创建一个子进程

- exec: 读取可执行文件，将其载入到内存中运行
  - exec() 在这里指所有 exec() 一族的函数。内核实现了 execve() 函数，在此基础上，还实现了 execlp()、 execle()、execv() 和 execvp()。



#### 3.1 写时拷贝

传统 fork() 系统调用：直接把所有的资源复制给新创建的进程。这种实现过于简单并且效率低下，因为它拷贝的数据也许并不共享，更糟的情况是，如果新进程打算立即执行一个新的映像，那么所有的拷贝都将前功尽弃。

**Linux 的 fork() 使用写时拷贝 (copy-on-write) 页实现**：

- 写时拷贝是一种可以推迟甚至免除拷贝数据的技术。内核此时井不复制整个进程地址空间，而是让父进程和子进程共享同一个拷贝。
- 只有在需要写入的时候，数据才会被复制，从而使各个进程拥有各自的拷贝。资源的复制只有在需要写入的时候才进行，在此之前，只是以只读方式共享
- 这种技术使地址空间上的页的拷贝被推迟到实际发生写入的时候才进行。在页根本不会被写入的情况下（举例来说，fork() 后立即调用 exec() 它们就无须复制了。

**fork() 的实际开销就是复制父进程的页表以及给子进程创建唯一的进程描述符**。在一般情况下，进程创建后都会马上运行一个可执行的文件，**这种优化可以避免拷贝大量根本就不会被使用的数据**（地址空间里常常包含数十兆的数据）。由于 Unix 强调进程快速执行的能力，所以这个优化是很重要的。



#### 3.2 fork()

**Linux 通过 clone() 系统调用实现 fork()**。这个调用通过一系列的参数标志来指明父、子进程需要共享的资源。 fork()、 vfork()和＿clone() 库函数都根据各自需要的参数标志去调用 clone()，然后由 clone() 去调用 do_fork()。

do_fork() 完成了创建中的大部分工作，它的定义在 kernel/fork.c 文件中。该函数调用 copy_process() 函数，然后让进程开始运行。 copy_process() 函数完成的工作如下：

1. 调用dup_task_struct() 为新进程分配内核栈、thread_info 和 task_struct 等，其中的内容与父进程相同。此时，子进程和父进程的描述符是完全相同的。
2. check新进程（进程数目是否超出上限等）
3. 清理新进程的信息（比如PID置0等），使之与父进程区别开。
4. 新进程状态置为 TASK_UNINTERRUPTIBLE，以保证它不会投入运行。
5. 调用 copy_ftags() 更新 task_struct 的 flags 成员。
6. 调用 alloc_pid() 为新进程分配一个有效的 PID
7. 根据clone()的参数标志，拷贝或共享相应的信息
8. 做一些扫尾工作并返回指向新进程的指针

copy_process() 函数执行完成后，返回到do_fork() 函数。若copy_process() 函数成功返回，新创建的子进程被唤醒井让其投入运行。

**内核有意选择子进程首先执行 。因为一般子进程都会马上调用 exec() 函数，这样可以避免写时拷贝的额外开销。**

**创建进程的fork()函数实际上最终是调用clone()函数。**



#### 3.3 vfork()

除了**不拷贝父进程的页表项**外， vfork() 系统调用和 fork() 的功能相同。子进程作为父进程的一个单独的线程在它的地址空间里运行，父进程被阻塞，**直到子进程退出或执行 exec()**。子进程不能向地址空间写入。

现在由于在执行 fork() 时引入了写时拷贝页井且明确了子进程先执行，vfork() 的好处就**仅限于不拷贝父进程的页表项**了。如果 Linux 将来 fork() 有了写时拷贝页表项，那么vfork() 就彻底没用了。

vfork() 系统调用的实现是通过向 clone() 系统调用传递一个特殊标志来进行的。



### 4 线程在 Linux 中的实现

Linux 实现线程的机制非常独特。从内核的角度来说，它并没有线程这个概念。 Linux 把所有的线程都当做进程来实现。内核井没有准备特别的调度算法或是定义特别的数据结构来表征线程。**线程仅仅被视为一个与其他进程共享某些资源的进程**。**每个线程都拥有唯一隶属于自己的 task_struct**，所以在内核中，它看起来就像是一个普通的进程（只是线程和其他一些进程共享某些资源，如地址空间）。

「轻最级进程」这种叫法本身就概括了 Linux 在此处与其他系统的差异：

- 在其他的系统中，相较于重量级的进程，线程被抽象成一种耗费较少资源，运行迅速的执行单元。
- 而对于 Linux 来说，它只是一种进程间共享资源的手段。



#### 4.1 创建线程

创建线程和进程的步骤一样，只是最终传给clone()函数的参数不同。

比如，通过一个普通的fork来创建进程，相当于：`clone(SIGCHLD, 0)`

创建一个和父进程共享地址空间，文件系统资源，文件描述符和信号处理程序的进程，即一个线程：`clone(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND, 0)`

传递给 clone() 的参数标志决定了新创建进程的行为方式和父子进程之间共享的资源种类。这些参数标志在 <linux/sched.h> 中定义。



#### 4.2 内核线程

在内核中创建的内核线程与普通的进程之间的区别在于：**内核线程没有独立的地址空间，它们只能在内核空间运行**，从来不切换到用户空间去。内核进程和普通进程一样，可以被调度，也可以被抢占。

内核线程也只能由其他内核线程创建，内核是通过从 kthreadd 内核进程中衍生出所有新的内核线程来自动处理这一点的。新的任务是由 kthread 内核进程通过 clone() 系统调用而创建的。

它们只在内核空间运行，从来不切换到用户空间去。内核进程和普通进程一样，可以被调度，也可以被抢占。

 

### 5. 进程的终止

#### 5.1 删除进程描述符

进程的终止靠 do_exit() （定义于kemel/exit.c) 来完成，和创建进程一样，终结一个进程同样有很多步骤：

1、子进程上的操作(do_exit)

1. 设置task_struct中的标识成员设置为PF_EXITING
2. 调用del_timer_sync() 删除内核定时器, 确保没有定时器在排队和运行
3. 调用exit_mm()释放进程占用的mm_struct
4. 调用sem__exit()，使进程离开等待IPC信号的队列
5. 调用exit_files() 和exit_fs()，释放进程占用的文件描述符和文件系统资源
6. 把task_struct的exit_code设置为进程的返回值
7. 调用exit_notify() 向父进程发送信号，并把自己的状态设为EXIT_ZOMBIE
8. 切换到新进程继续执行

子进程进入EXIT_ZOMBIE之后，虽然永远不会被调度，关联的资源也释放掉了，但是它本身占用的内存还没有释放，比如创建时分配的内核栈、thread_info及task_struct结构等。这些由父进程来释放。此时进程存在的唯一目的就是向它的父进程提供信息。父进程检索到信息后，或者通知内核那是无关的信息后，由进程所持有的剩余内
存被释放，归还给系统使用。



在调用了 do_exit() 之后，尽管线程已经僵死不能再运行了，但是**系统还保留了它的进程描述符**。前面说过，这样做可以让系统有办法在子进程终结后仍能获得它的信息。因此，**进程终结时所需的清理工作和进程描述符的删除被分开执行**。在父进程获得已终结的子进程的信息后，或者通知内核它并不关注那些信息后，子进程的 task_struct 结构才被释放。



2、父进程上的操作(release_task)

父进程收到子进程发送的exit_notify()信号后，将该子进程的进程描述符和所有进程独享的资源全部删除。

1. 它调用\_exit_signal(），该函数调用\_unhash\_process(），后者又调用 detach_pid() 从 pidhash
   上删除该进程，同时也要从任务列表中删除该进程。
2. \_exit_signal() 释放目前僵死进程所使用的所有剩余资源，井进行最终统计和记录。
3. 如果这个进程是线程组最后一个进程，并且领头进程已经死掉，那么 release_task() 就要通知僵死的领头进程的父进程。
4. release_task()调用 put_task_struct() 释放进程内核栈和 thread_info 结构所占的页，井释放tast_struct 所占的 slab 高速缓存。

至此，进程描述符和所有进程独享的资源就全部释放掉了。



#### 5.2 孤儿进程造成的进退维谷

如果父进程在子进程之前退出，必须有机制来保证子进程能找到一个新的父亲，否则这些成为孤儿的进程就会在退出时永远处于僵死状态，白白地耗费内存。

解决方法是**给子进程在当前线程组内找一个线程作为父亲，如果不行，就让 init 做它们的父进程**。在 do_exit(）中会调用 exit_notify()，该函数会调用 forget_original__parent()，而后者会调用 find_new _reaper() 来执行寻父过程。

find_new_reaper()函数先在当前线程组中找一个线程作为父亲，如果找不到，就让init做父进程。(init进程是在linux启动时就一直存在的)

一旦系统为进程成功地找到和设置了新的父进程，就不会再有出现驻留僵死进程的危险了。init 进程会例行调用 wait( ）来检查其子进程，清除所有与其相关的僵死进程。



# 第四章 进程调度

调度程序负责**决定将哪个进程投入运行，何时运行以及运行多长时间**。进程调度程序（常常简称调度程序）可看做**在可运行态进程之间分配有限的处理器时间资源的内核子系统**。调度程序是像 Linux 这样的多任务操作系统的基础。只有通过调度程序的合理调度，系统资源才能最大限度地发挥作用，多进程才会有并发执行的效果。

最大限度地利用处理器时间的原则是，**只要有可以执行的进程，那么就总会有进程正在执行**。但是只要系统中可运行的进程的数目比处理器的个数多，就注定某一给定时刻会有一些进程不能执行。这些进程在**等待运行**。**在一组处于可运行状态的进程中选择一个来执行，是调度程序所需完成的基本工作**。



### 1. 多任务

 多任务操作系统就是能同时井发地交互执行多个进程的操作系统。在单处理器机器上，这会产生多个进程在同时运行的幻觉。在多处理器机器上，这会使多个进程在不同的处理机上真正同时、并行地运行。

无论在单处理器或者多处理器机器上，多任务操作系统都能使多个进程处于堵塞或者睡眠状态，也就是说，实际上不被投入执行，直到工作确实就绪。这些任务尽管位于内存，但井不处于可运行状态。相反，这些进程利用内核阻塞自己，直到某一事件（键盘输人、网
络数据、过一段时间等）发生。

多任务系统可以划分为两类：

1. 非抢占式多任务

2. 抢占式多任务

**Linux 提供了抢占式的多任务模式**。在此模式下，**由调度程序来决定什么时候停止一个进程的运行，以便其他进程能够得到执行机会**。这个强制的挂起动作就叫做**抢占** (preemption) 。进程在被抢占之前能够运行的时间是预先设置好的，而且有一个专门的名字，叫进程的**时间片** (timeslice) 。时间片实际上就是分配给每个可运行进程的处理器时间段。有效管理时间片能使调度程序从系统全局的角度做出调度决定，这样做还可以避免个别进程独占系统资源。当今众多现代操作系统对程序运行都采用了动态时间片计算的方式，并且引入了可配置的计算策略。

在非抢占式多任务模式下，除非进程自己主动停止运行，否则它会一直执行。进程主动挂起自己的操作称为**让步** (yielding)。理想情况下，进程通常做出让步，以便让每个可运行进程享有足够的处理器时间。但这种机制**有很多缺点**：调度程序无法对每个进程该执行多长时间做出统一规定，所以进程独占的处理器时间可能超出用户的预料；更糟的是，一个决不做出让步的悬挂进程就能使系统崩溃。



### 2. Linux的进程调度

Linux 2.5 引入 O(1) 调度程序，其使用了静态时间片算法和针对每一处理器的运行队列。

Linux 2.6 引入了新的进程调度算法。其中最为著名的是 “反转楼梯最后期限调度算法“（RSDL），该算法吸取了队列理论，将公平调度的概念引入了 Linux 调度程序。并且最终在 2.6.23 内核版本中替代了 O(1) 调度算法，它此刻被称为 “**完全公平调度算法**”，或者简称 CFS。



### 3. 策略

**策略决定调度程序在何时让什么进程运行**。调度器的策略往往就决定系统的整体印象，并且还负责优化使用处理器时间，是至关重要的。



#### 3.1 I/O 消耗型和处理器消耗型的进程

**进程可以被分为 I/O 消耗型和处理器消耗型**：

- I/O 消耗型指进程的**大部分时间用来提交 I/O 请求或是等待 I/O 请求**。因此，这样的进程经常处于可运行状态，但通常都是运行短短的一会儿，因为它在等待更多的 I/O 请求时最后总会阻塞。
- 处理器耗费型进程把**时间大多用在执行代码上**。除非被抢占，否则它们通常都一直不停地运行，因为它们没有太多的 1/0 需求。但是，因为它们不属千 1/0 驱动类型，所以从系统响应速度考虑，调度器不应该经常让它们运行。对千这类处理器消耗型的进程，调度策略往往是尽量降低它们的调度频率，而延长其运行时间。处理器消耗型进程的极端例子就是无限循环地执行。

当然，这种划分方法并非是绝对的。**进程可以同时展示这两种行为**：比如， X Window 服务器既是 I/O 消耗型，也是处理器消耗型。还有些进程可以是 I/O 消耗型，但属于处理器消耗型活动的范围。其典型的例子就是字处理器，其通常坐以等待键盘输入，但在任一时刻可能又粘住处理器疯狂地进行拼写检查或者宏计算。

调度策略通常要在两个矛盾的目标中间寻找平衡：**进程响应迅速（响应时间短）和最大系统利用率（高吞吐量）。**为了满足上述需求，调度程序通常采用一套非常复杂的算法来决定最值得运行的进程投入运行，但是它**往往并不保证低优先级进程会被公平对待**。Linux 为了保证交互式应用和桌面系统的性能，所以对进程的响应做了优化（缩短响应时间），**更倾向于优先调度 I/O 消耗型进程**。虽然如此，但在下面你会看到，调度程序也并未忽略处理器消耗型的进程。



#### 3.2 进程优先级（此处开始未作笔记）

前面说过，调度功能就是决定哪个进程运行以及进程运行多长时间。

决定哪个进程运行以及运行多长时间都和进程的优先级有关。为了确定一个进程到底能持续运行多长时间，调度中还引入了时间片的概念。

#### 

进程的优先级有2种度量方法，一种是nice值，一种是实时优先级。

nice值的范围是-20～+19，值越大优先级越低，也就是说nice值为-20的进程优先级最大。

实时优先级的范围是0～99，与nice值的定义相反，实时优先级是值越大优先级越高。

实时进程都是一些对响应时间要求比较高的进程，因此系统中有实时优先级高的进程处于运行队列的话，它们会抢占一般的进程的运行时间。

 

进程的2种优先级会让人不好理解，到底哪个优先级更优先？一个进程同时有2种优先级怎么办？

其实linux的内核早就有了解决办法。

对于第一个问题，到底哪个优先级更优先？

答案是实时优先级高于nice值，在内核中，实时优先级的范围是 0～MAX_RT_PRIO-1 MAX_RT_PRIO的定义参见 include/linux/sched.h

```
1611 #define MAX_USER_RT_PRIO        100
1612 #define MAX_RT_PRIO             MAX_USER_RT_PRIO
```

nice值在内核中的范围是 MAX_RT_PRIO～MAX_RT_PRIO+40 即 MAX_RT_PRIO～MAX_PRIO

```
1614 #define MAX_PRIO                (MAX_RT_PRIO + 40)
```

 

第二个问题，一个进程同时有2种优先级怎么办？

答案很简单，就是一个进程不可能有2个优先级。一个进程有了实时优先级就没有Nice值，有了Nice值就没有实时优先级。

我们可以通过以下命令查看进程的实时优先级和Nice值：(其中RTPRIO是实时优先级，NI是Nice值)

```
$ ps -eo state,uid,pid,ppid,rtprio,ni,time,comm
S   UID   PID  PPID RTPRIO  NI     TIME COMMAND
S     0     1     0      -   0 00:00:00 systemd
S     0     2     0      -   0 00:00:00 kthreadd
S     0     3     2      -   0 00:00:00 ksoftirqd/0
S     0     6     2     99   - 00:00:00 migration/0
S     0     7     2     99   - 00:00:00 watchdog/0
S     0     8     2     99   - 00:00:00 migration/1
S     0    10     2      -   0 00:00:00 ksoftirqd/1
S     0    12     2     99   - 00:00:00 watchdog/1
S     0    13     2     99   - 00:00:00 migration/2
S     0    15     2      -   0 00:00:00 ksoftirqd/2
S     0    16     2     99   - 00:00:00 watchdog/2
S     0    17     2     99   - 00:00:00 migration/3
S     0    19     2      -   0 00:00:00 ksoftirqd/3
S     0    20     2     99   - 00:00:00 watchdog/3
S     0    21     2      - -20 00:00:00 cpuset
S     0    22     2      - -20 00:00:00 khelper
```

 

#### 3.3 时间片

有了优先级，可以决定谁先运行了。但是对于调度程序来说，并不是运行一次就结束了，还必须知道间隔多久进行下次调度。

于是就有了时间片的概念。时间片是一个数值，表示一个进程被抢占前能持续运行的时间。

也可以认为是进程在下次调度发生前运行的时间(除非进程主动放弃CPU，或者有实时进程来抢占CPU)。

时间片的大小设置并不简单，设大了，系统响应变慢(调度周期长)；设小了，进程频繁切换带来的处理器消耗。默认的时间片一般是10ms

 

#### 2.3 调度实现原理（基于优先级和时间片）

下面举个直观的例子来说明：

假设系统中只有3个进程ProcessA(NI=+10)，ProcessB(NI=0)，ProcessC(NI=-10)，NI表示进程的nice值，时间片=10ms

1. 调度前，把进程优先级按一定的权重映射成时间片(这里假设优先级高一级相当于多5msCPU时间)。

   假设ProcessA分配了一个时间片10ms，那么ProcessB的优先级比ProcessA高10(nice值越小优先级越高)，ProcessB应该分配10\*5+10=60ms，以此类推，ProcessC分配20\*5+10=110ms

2. 开始调度时，优先调度分配CPU时间多的进程。由于ProcessA(10ms),ProcessB(60ms),ProcessC(110ms)。显然先调度ProcessC

3. 10ms(一个时间片)后，再次调度时，ProcessA(10ms),ProcessB(60ms),ProcessC(100ms)。ProcessC刚运行了10ms，所以变成100ms。此时仍然先调度ProcessC

4. 再调度4次后(4个时间片)，ProcessA(10ms),ProcessB(60ms),ProcessC(60ms)。此时ProcessB和ProcessC的CPU时间一样，这时得看ProcessB和ProcessC谁在CPU运行队列的前面，假设ProcessB在前面，则调度ProcessB

5. 10ms(一个时间片)后，ProcessA(10ms),ProcessB(50ms),ProcessC(60ms)。再次调度ProcessC

6. ProcessB和ProcessC交替运行，直至ProcessA(10ms),ProcessB(10ms),ProcessC(10ms)。

     这时得看ProcessA，ProcessB，ProcessC谁在CPU运行队列的前面就先调度谁。这里假设调度ProcessA

7. 10ms(一个时间片)后，ProcessA(时间片用完后退出),ProcessB(10ms),ProcessC(10ms)。

8. 再过2个时间片，ProcessB和ProcessC也运行完退出。



这个例子很简单，主要是为了说明调度的原理，实际的调度算法虽然不会这么简单，但是基本的实现原理也是类似的：

1. 确定每个进程能占用多少CPU时间(这里确定CPU时间的算法有很多，根据不同的需求会不一样)

2. 占用CPU时间多的先运行

3. 运行完后，扣除运行进程的CPU时间，再回到 1）

 

### **3. Linux上调度实现的方法**

Linux上的调度算法是不断发展的，在2.6.23内核以后，采用了“完全公平调度算法”，简称CFS。

CFS算法在分配每个进程的CPU时间时，不是分配给它们一个绝对的CPU时间，而是根据进程的优先级分配给它们一个占用CPU时间的百分比。

比如ProcessA(NI=1)，ProcessB(NI=3)，ProcessC(NI=6)，在CFS算法中，分别占用CPU的百分比为：ProcessA(10%)，ProcessB(30%)，ProcessC(60%)

因为总共是100%，ProcessB的优先级是ProcessA的3倍，ProcessC的优先级是ProcessA的6倍。

 

Linux上的CFS算法主要有以下步骤：(还是以ProcessA(10%)，ProcessB(30%)，ProcessC(60%)为例)

1. 计算每个进程的vruntime(注1)，通过update_curr()函数更新进程的vruntime。

2. 选择具有最小vruntime的进程投入运行。（注2）

3. 进程运行完后，更新进程的vruntime，转入步骤2) （注3）

 

**注1.** 这里的vruntime是进程虚拟运行的时间的总和。vruntime定义在：kernel/sched_fair.c 文件的 struct sched_entity 中。

 

**注2.** 这里有点不好理解，根据vruntime来选择要运行的进程，似乎和每个进程所占的CPU时间百分比没有关系了。

1. 比如先运行ProcessC，(vr是vruntime的缩写)，则10ms后：ProcessA(vr=0)，ProcessB(vr=0)，ProcessC(vr=10)

2. 那么下次调度只能运行ProcessA或者ProcessB。(因为会选择具有最小vruntime的进程)

长时间来看的话，ProcessA、ProcessB、ProcessC是公平的交替运行的，和优先级没有关系。

而实际上**vruntime**并不是实际的运行时间，它是**实际运行时间进行加权运算**后的结果。

比如上面3个进程中ProcessA(10%)只分配了CPU总的处理时间的10%，那么ProcessA运行10ms的话，它的vruntime会增加100ms。

以此类推，ProcessB运行10ms的话，它的vruntime会增加(100/3)ms,ProcessC运行10ms的话，它的vruntime会增加(100/6)ms。

实际的运行时，由于ProcessC的vruntime增加的最慢，所以它会获得最多的CPU处理时间。

上面的加权算法是我自己为了理解方便简化的，Linux对vruntime的加权方法还得去看源码^-^

 

**注3.**Linux为了能快速的找到具有最小vruntime，将所有的进程的存储在一个红黑树中。这样树的最左边的叶子节点就是具有最小vruntime的进程，新的进程加入或有旧的进程退出时都会更新这棵树。

 

其实Linux上的调度器是以模块方式提供的，每个调度器有不同的优先级，所以可以同时存在多种调度算法。

每个进程可以选择自己的调度器，Linux调度时，首先按调度器的优先级选择一个调度器，再选择这个调度器下的进程。

 

### **4. 调度相关的系统调用**

调度相关的系统调用主要有2类：

1) 与调度策略和进程优先级相关 (就是上面的提到的各种参数，优先级，时间片等等) - 下表中的前8个

2) 与处理器相关 - 下表中的最后3个

| **系统调用**             | **描述**                                                     |
| ------------------------ | ------------------------------------------------------------ |
| nice()                   | 设置进程的nice值                                             |
| sched_setscheduler()     | 设置进程的调度策略，即设置进程采取何种调度算法               |
| sched_getscheduler()     | 获取进程的调度算法                                           |
| sched_setparam()         | 设置进程的实时优先级                                         |
| sched_getparam()         | 获取进程的实时优先级                                         |
| sched_get_priority_max() | 获取实时优先级的最大值，由于用户权限的问题，非root用户并不能设置实时优先级为99 |
| sched_get_priority_min() | 获取实时优先级的最小值，理由与上面类似                       |
| sched_rr_get_interval()  | 获取进程的时间片                                             |
| sched_setaffinity()      | 设置进程的处理亲和力，其实就是保存在task_struct中的cpu_allowed这个掩码标志。该掩码的每一位对应一个系统中可用的处理器，默认所有位都被设置，即该进程可以再系统中所有处理器上执行。用户可以通过此函数设置不同的掩码，使得进程只能在系统中某一个或某几个处理器上运行。 |
| sched_getaffinity()      | 获取进程的处理亲和力                                         |
| sched_yield()            | 暂时让出处理器                                               |



# 第十章 内核同步方法

## 1. 原子操作



## 2. 自旋锁

自旋锁最多只能被一个可执行线程持有。如果一个执行线程试图获得一个被已经持有（即所谓的争用）的自旋锁，那么该线程就会一直进行**忙循环**—旋转—等待锁重新可用。

因为自旋锁在同一时刻至多被一个执行线程持有，所以一个时刻只能有一个线程位于临界区内，这就为多处理器机器提供了防止并发访问所需的保护机制。

一个被争用的自旋锁**使得请求它的线程在等待锁重新可用时自旋**（特别浪费处理器时间），这种行为是自旋锁的要点。

注意：**自旋锁**在发生争用时，使得等待的线程进行**忙等待**；而**信号量**使得等待的线程能投入**睡眠**，不是旋转。



注意：**自旋锁是不可递归的！！！**

> Linux 内核实现的自旋锁是不可递归的，这点不同于自旋锁在其他操作系统中的实现。所以如果你试图得到一个你正持有的锁，你必须自旋，等待你自己释放这个锁。但你处于自旋忙等待中，所以你永远没有机会释放锁，于是你被自己锁死了。千万小心自旋锁！



注意：**自旋锁可以在中断处理程序中使用**，**但信号量不可在中断中使用**，因为它会导致睡眠！！！



注意：**在中断处理程序中使用自旋锁时，一定要在获取锁之前，首先禁止本地中断（在当前处理器上的中断请求）！！！**

> 否则，中断处理程序就会打断正持有锁的内核代码，有可能会试图去争用这个已被持有的自旋锁。这样一来，中断处理程序就会自旋，等待该锁重新可用，但是锁的持有者在这个中断处理程序执行完毕前不可能运行。这正是我们在前面的内容中提到的双重请求死锁。注意，需要关闭的只是当前处理器上的中断。如果中断发生在不同的处理器上，即使中断处理程序在同一锁上自旋，也不会妨碍锁的持有者（在不同处理器上）最终释放锁。



### 自旋锁使用方法

------

自旋锁的基本使用形式：

```c
DEFINE_SPINLOCK(mr_lock); 
spin_lock(&mr_lock); 
/*临界区...*/
spin_unlock(&mr_lock) ; 
```



内核还提供了禁止中断同时请求锁的接口：

```c
DEFINE_SPINLOCK(mr_lock); 
unsigned long flags; 

spin_lock_irqsave(&mr_lock,flags); 
/*临界区...*/
spin_unlock_irqrestore(&mr_lock,flags);
```

> 函数 spin_lock_ irqsave() 保存中断的当前状态，井禁止本地中断，然后再去获取指定的锁。反过来 spin_unlock_ irqrestore() 对指定的锁解锁，然后让中断恢复到加锁前的状态。



如果你能确定中断在加锁前是激活的，那就不需要在解锁后恢复中断以前的状态了。你可以无条件地在解锁时激活中断。这时，使用 spin_lock_irq() 和 spin_unlock_irq() 会更好一些：

```c
DEFINE_SPINLOCK(mr_lock); 
spin_lock_irq(&mr_lock); 
/*临界区...*/
spin_unlock_irq(&mr_lock) ; 
```



自旋锁方法列表如下：

| **方法**               | **描述**                                             |
| ---------------------- | ---------------------------------------------------- |
| spin_lock()            | 获取指定的自旋锁                                     |
| spin_lock_irq()        | 禁止本地中断并获取指定的锁                           |
| spin_lock_irqsave()    | 保存本地中断的当前状态，禁止本地中断，并获取指定的锁 |
| spin_unlock()          | 释放指定的锁                                         |
| spin_unlock_irq()      | 释放指定的锁，并激活本地中断                         |
| spin_unlock_irqstore() | 释放指定的锁，并让本地中断恢复到以前状态             |
| spin_lock_init()       | 动态初始化指定的spinlock_t                           |
| spin_trylock()         | 试图获取指定的锁，如果未获取，则返回0                |
| spin_is_locked()       | 如果指定的锁当前正在被获取，则返回非0，否则返回0     |



### 自旋锁和下半部

------

中断处理下半部的操作中使用自旋锁尤其需要小心：

1. 下半部处理和进程上下文共享数据时，由于下半部的处理可以抢占进程上下文的代码， 所以进程上下文在对共享数据加锁前要禁止下半部的执行，解锁时再允许下半部的执行。
2. 中断处理程序（上半部）和下半部处理共享数据时，由于中断处理（上半部）可以抢占下半部的执行， 所以下半部在对共享数据加锁前要禁止中断处理（上半部），解锁时再允许中断的执行。
3. 同一种tasklet不能同时运行，所以同类tasklet中的共享数据不需要保护。
4. 不同类tasklet中共享数据时，其中一个tasklet获得锁后，不用禁止其他tasklet的执行，因为同一个处理器上不会有tasklet相互抢占的情况
5. 同类型或者非同类型的软中断在共享数据时，也不用禁止下半部，因为同一个处理器上不会有软中断互相抢占的情况



## 3. 读-写自旋锁



## 4. 信号量



## 5. 读写信号量



## 6. 互斥体



## 7. 完成变量



## 8. 大内核锁-BLK



## 9. 顺序锁